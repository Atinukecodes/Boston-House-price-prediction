# -*- coding: utf-8 -*-
"""Housing Price Predtiction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WC0Gwa0MuG_-Pt1ViypwueGhAe4G1cyn

# Hi There
Today, the 29th of September, 2025, I will be analysing the Boston Housing dataset by predicting the house prices based on the provided features in the dataset.

Join me in this ride as we analyze and work with numbers to help realtors in the housing industry.

**1. Importing all necessary libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import mutual_info_regression

"""**2. Dataset Overview**"""

df_dirty = pd.read_csv('/content/drive/MyDrive/BostonHousing (1).csv')
df_dirty.head()

rows=df_dirty.shape[0]
columns=df_dirty.shape[1]
print(f'The Boston Housing Datset has {rows} rows and {columns} columns')

"""From the above line of code, there are 506 rows and 14 columns in the dataset.

Now, I want to see what these feautres are made up of
"""

df_dirty.info()

"""From the result of the above block of code, we see that all features are numeric and the only column with missing values is the RM (Average number of rooms per dwelling) column.

Now, I want to check for missing values or duplicate values
"""

df_dirty.isnull().sum()

"""There are only 5 missing values, all in RM column

Now, for duplicate values
"""

df_dirty.duplicated().sum()

"""No duplicates. Pheew!!!

**Data Cleaning**

Ok, since the rm column is a numerical column, I want to fill the empty cells with the mean.
"""

df_dirty.describe()

df_dirty['rm'].fillna(df_dirty['rm'].mean(), inplace=True)

# Rename the cleaned dataset
df_clean = df_dirty.copy()

# Print the new dataset

df_clean.to_csv('BostonHousing_clean.csv', index=False)

df_clean.head()

df_clean.info()

df_clean.isnull().sum().sum()

"""Great, now the dataset is clean.

**Feature Engineering 1**

In this stage, I want to make use of MI (Mutual Information) Score to get the top 5 features that influence the MEDV (Median House Price), to further build a model.
"""

# Separate features and target
X = df_clean.drop('medv', axis=1)
y = df_clean['medv']

# Compute MI scores
mi_scores = mutual_info_regression(X, y)
mi_scores_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)

# Display top 5 features
top_features = mi_scores_series.head(5).index.tolist()
print("Top 5 features affecting MEDV:", top_features)

"""Now, using Linear Regression to build a model

**Train-Test Split**
"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# Select top 5 features
X_top = df_clean[top_features]


# Split data
X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**Model Training**"""

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

"""After training the model, It's time to produce a Linear Regression equation that each of the features in my dataset would be inputted to produce a prediction.

 Afterwards, the prediction would be compared to the actual house price, in order to know how well the model performed
"""

# Coefficients and intercept
coefficients = model.coef_
intercept = model.intercept_

# Print regression equation
equation = "MEDV = " + " + ".join([f"{coef:.2f}*{feature}" for coef, feature in zip(coefficients, top_features)])
equation = f"{equation} + {intercept:.2f}"
print("Regression Equation:", equation)

"""**Model Evaluation**"""

# Predictions on training set
y_train_pred = model.predict(X_train)

# Training evaluation
r2_train = r2_score(y_train, y_train_pred)
mse_train = mean_squared_error(y_train, y_train_pred)

print(f"Training RÂ² Score: {r2_train:.4f}")
print(f"Training Mean Squared Error (MSE): {mse_train:.4f}")

"""*RÂ² measures the proportion of the variance in the target variable (housing prices) that is explained by the model. It ranges from 0 to 1*"""

# Predictions on test set
y_test_pred = model.predict(X_test)

# Test evaluation
r2_test = r2_score(y_test, y_test_pred)
mse_test = mean_squared_error(y_test, y_test_pred)

print(f"Test RÂ² Score: {r2_test:.4f}")
print(f"Test Mean Squared Error (MSE): {mse_test:.4f}")

"""*Residual Plot*"""

import matplotlib.pyplot as plt
import seaborn as sns

# Residuals
residuals = y_test - y_test_pred

# Plot
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test_pred, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted MEDV")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

"""If Residual (Actual - Predicted) = 0, the model predicted perfectly

If Residual < 0, the model overpredicted

If Residual > 0, the model underpredicted

*Actual vs Predicted Plot*
"""

plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test, y=y_test_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel("Actual MEDV")
plt.ylabel("Predicted MEDV")
plt.title("Actual vs Predicted MEDV")
plt.show()

"""For this graph, red line is the state where the actual values are equal to the predicted values.

**Now, using Random Forest Regressor for the top 5 features**
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred_rf = rf_model.predict(X_train)
y_test_pred_rf = rf_model.predict(X_test)

# Training evaluation
r2_train_rf = r2_score(y_train, y_train_pred_rf)
mse_train_rf = mean_squared_error(y_train, y_train_pred_rf)

# Testing evaluation
r2_test_rf = r2_score(y_test, y_test_pred_rf)
mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)

# Print results
print("Training Set Evaluation:")
print(f"RÂ² Score: {r2_train_rf:.4f}")
print(f"Mean Squared Error: {mse_train_rf:.4f}\n")

print("Test Set Evaluation:")
print(f"RÂ² Score: {r2_test_rf:.4f}")
print(f"Mean Squared Error: {mse_test_rf:.4f}")

import matplotlib.pyplot as plt

# Assuming y_test and y_pred are available from your test set predictions
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # Diagonal line
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs. Predicted Prices (Test Set)')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate residuals
residuals_rf = y_test - y_test_pred_rf

# Plot residuals
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test_pred_rf, y=residuals_rf)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted MEDV (Random Forest)")
plt.ylabel("Residuals")
plt.title("Residual Plot - Random Forest")
plt.show()

"""**Now, using Lasso and Ridge models for the top 5 features**"""

from sklearn.linear_model import Lasso, Ridge
from sklearn.metrics import r2_score, mean_squared_error

# Initialize models
lasso = Lasso(alpha=0.1)
ridge = Ridge(alpha=1.0)

# Fit models
lasso.fit(X_train, y_train)
ridge.fit(X_train, y_train)

# Predict
y_train_pred_lasso = lasso.predict(X_train)
y_test_pred_lasso = lasso.predict(X_test)

y_train_pred_ridge = ridge.predict(X_train)
y_test_pred_ridge = ridge.predict(X_test)

# Lasso Evaluation
r2_train_lasso = r2_score(y_train, y_train_pred_lasso)
mse_train_lasso = mean_squared_error(y_train, y_train_pred_lasso)
r2_test_lasso = r2_score(y_test, y_test_pred_lasso)
mse_test_lasso = mean_squared_error(y_test, y_test_pred_lasso)

# Ridge Evaluation
r2_train_ridge = r2_score(y_train, y_train_pred_ridge)
mse_train_ridge = mean_squared_error(y_train, y_train_pred_ridge)
r2_test_ridge = r2_score(y_test, y_test_pred_ridge)
mse_test_ridge = mean_squared_error(y_test, y_test_pred_ridge)

# Print results
print(" Lasso Regression:")
print(f"Training RÂ²: {r2_train_lasso:.4f}, MSE: {mse_train_lasso:.4f}")
print(f"Testing RÂ²: {r2_test_lasso:.4f}, MSE: {mse_test_lasso:.4f}\n")

print(" Ridge Regression:")
print(f"Training RÂ²: {r2_train_ridge:.4f}, MSE: {mse_train_ridge:.4f}")
print(f"Testing RÂ²: {r2_test_ridge:.4f}, MSE: {mse_test_ridge:.4f}")

# Create a list to store results
results = []

# Define models
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor

models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Lasso Regression': Lasso(alpha=0.1),
    'Ridge Regression': Ridge(alpha=1.0)
}

# Use top 5 features (assuming you already have top_features)
X_top = df_clean[top_features]
y = df_clean['medv']

# Split data
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)

# Loop through models
for name, model in models.items():
    model.fit(X_train, y_train)

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Metrics
    r2_train = r2_score(y_train, y_train_pred)
    mse_train = mean_squared_error(y_train, y_train_pred)
    r2_test = r2_score(y_test, y_test_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Store results
    results.append({
        'Model': name,
        'RÂ² Train': round(r2_train, 4),
        'MSE Train': round(mse_train, 4),
        'RÂ² Test': round(r2_test, 4),
        'MSE Test': round(mse_test, 4)
    })

# Convert to DataFrame and print
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df)

"""# Conclusion

**Key Insights**

 **Best Performing Model:** Random Forest
Highest RÂ² on both training (0.9744) and test (0.8149) sets.

Lowest MSE on both training (2.2202) and test (13.5740) sets.

This model captures complex relationships in the data far better than the others.

 Linear, Lasso, and Ridge Regression
All three have similar performance, with RÂ² around 0.68 (train) and 0.63 (test).

Their MSEs are also close, hovering around 27.

These models are simpler and likely underfitting the dataâ€”theyâ€™re not capturing enough complexity.

Conclusion
Most Accurate & Best Generalizing Model:

 **Random Forest**

It balances high training accuracy with strong test performance, indicating good generalization.

Least Effective Models: Linear, Lasso, and Ridge

Their performance is nearly identical and significantly lower than Random Forest.

They may be more interpretable, but they sacrifice predictive power.

**Now, to use correlation matrix to drop highly correlated features, and then build models with the result**
"""

# Full feature set
X_full = df_clean.drop('medv', axis=1)

# Split both sets
X_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)
X_train_red, X_test_red, _, _ = train_test_split(X_reduced, y, test_size=0.2, random_state=42)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Compute correlation matrix
corr_matrix = df_clean.corr()

# Set up the plot
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)

# Add title
plt.title("Feature Correlation Heatmap")
plt.show()

# Compute correlation matrix
corr_matrix = df_clean.drop('medv', axis=1).corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Drop features with correlation > 0.8
to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]
print("Dropped features due to high correlation:", to_drop)

# Reduced feature set
X_reduced = df_clean.drop(columns=to_drop + ['medv'])
y = df_clean['medv']

# Define models
models = {
    'Linear': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
}

# Store results
results = []

# Loop through models and datasets
for name, model in models.items():
    for version, X_train, X_test in [('Full', X_train_full, X_test_full), ('Reduced', X_train_red, X_test_red)]:
        model.fit(X_train, y_train)
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        r2_train = r2_score(y_train, y_train_pred)
        mse_train = mean_squared_error(y_train, y_train_pred)
        r2_test = r2_score(y_test, y_test_pred)
        mse_test = mean_squared_error(y_test, y_test_pred)

        results.append({
            'Model': name,
            'Feature Set': version,
            'RÂ² Train': round(r2_train, 4),
            'MSE Train': round(mse_train, 4),
            'RÂ² Test': round(r2_test, 4),
            'MSE Test': round(mse_test, 4)
        })

# Convert to DataFrame
results_df = pd.DataFrame(results)
print(results_df)

"""Model-by-Model Breakdown
1. Linear Regression
Full Features: RÂ² Test = 0.6672, MSE Test = 24.4048

Reduced Features: Slight drop in performance (RÂ² = 0.6516, MSE = 25.5492)

Insight: Linear regression performs moderately well. Dropping correlated features slightly hurts performance, suggesting those features were contributing useful information.

Ridge Regression
Full Features: RÂ² Test = 0.6648, MSE Test = 24.5791

Reduced Features: RÂ² = 0.6490, MSE = 25.7391

Insight: Ridge is nearly identical to Linear Regression. It adds regularization to prevent overfitting, but in this case, the benefit is minimal. Again, reducing features slightly degrades performance.

3. Lasso Regression
Full Features: RÂ² Test = 0.6559, MSE Test = 25.2312

Reduced Features: RÂ² = 0.6371, MSE = 26.6092

Insight: Lasso performs slightly worse than Ridge and Linear. Itâ€™s more aggressive in shrinking coefficients, which can help with feature selectionâ€”but here, it may be discarding useful information.

4. Random Forest
Full Features: RÂ² Test = 0.8929, MSE Test = 7.8535

Reduced Features: RÂ² = 0.8892, MSE = 8.1279

Insight: Random Forest is the clear winner. It captures complex, nonlinear relationships that linear models miss. Even after dropping correlated features, it maintains excellent performanceâ€”showing robustness and generalization.

Key Takeaways

 Best Model Overall:

# **Random Forest**

Highest RÂ² and lowest MSE on both training and test sets.

Handles full and reduced feature sets with minimal performance loss.

Ideal for predictive accuracy, especially when interpretability isnâ€™t the top priority.

Linear, Ridge, and Lasso


These models are simpler and more interpretable.

They struggle to capture complex patterns in your data.

Performance drops slightly when features are reducedâ€”suggesting that correlated features were still informative.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Your results data
results_data = {
    'Model': ['Linear', 'Linear', 'Ridge', 'Ridge', 'Lasso', 'Lasso', 'RandomForest', 'RandomForest'],
    'Feature Set': ['Full', 'Reduced'] * 4,
    'RÂ² Train': [0.7504, 0.7463, 0.7483, 0.7440, 0.7377, 0.7323, 0.9770, 0.9765],
    'RÂ² Test': [0.6672, 0.6516, 0.6648, 0.6490, 0.6559, 0.6371, 0.8929, 0.8892]
}

df = pd.DataFrame(results_data)

# Plot RÂ² Test Scores
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='RÂ² Test', hue='Feature Set', data=df, palette='viridis')
plt.title("RÂ² Test Score Comparison")
plt.ylabel("RÂ² Score")
plt.ylim(0.6, 1.0)
plt.legend(title='Feature Set')
plt.show()

# Add MSE data
df['MSE Test'] = [24.4048, 25.5492, 24.5791, 25.7391, 25.2312, 26.6092, 7.8535, 8.1279]

# Plot MSE Test Scores
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='MSE Test', hue='Feature Set', data=df, palette='magma')
plt.title("MSE Test Score Comparison")
plt.ylabel("Mean Squared Error")
plt.legend(title='Feature Set')
plt.show()

"""Simply put, if the RÂ² is closer to one and the MSE score is lower, it signifies predictive accuracy.

In this analysis, **Random Forest** has those features, and that is why it stands out

There you have it.

# I remain, Miss Atinuke Towoju

Stay blessedðŸ˜Š
"""